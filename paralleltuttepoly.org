#+title: Parallel Computation of the Tutte Polynomial
#+subtitle: 15-418: Project
#+author: Akira Kyle
#+date: April 18, 2018
#+email: akyle@cmu.edu
#+options: toc:nil email:t
#+latex_header_extra: \pagestyle{fancy}
#+latex_header_extra: \fancyhead[R]{Akira Kyle}
#+latex_header_extra: \fancyhead[L]{15-418}
#+latex_header_extra: \fancyhead[C]{Project Proposal}
#+latex_header_extra: \fancyfoot[C]{\thepage}

* Links
** Tutte background
[[https://en.wikipedia.org/wiki/Tutte_polynomial]]
Brendan McKay Nauty and Traces [[http://pallini.di.uniroma1.it/]]
Haggard C++ tuttepoly code [[http://homepages.ecs.vuw.ac.nz/~djp/tutte/]]
Mathematica tutte polynomial function [[http://reference.wolfram.com/language/ref/TuttePolynomial.html]]
Monagan maple tutte polynomial code [[http://www.cecm.sfu.ca/~mmonagan/tutte/]]
Vertex-exponential time tutte polynomial [[https://github.com/thorehusfeldt/tutte_bhkk]]
Blog post using tutte_bhkk in python [[http://mhenderson.net/page3/]]
** Five-flow
https://symomega.wordpress.com/2010/06/13/roots-of-flow-polynomials-and-welshs-conjecture/

** Fortran
http://www.featflow.de/en/software/featflow2/tutorial/tutorial_lang.html
https://en.wikibooks.org/wiki/Fortran
http://www.cs.rpi.edu/~szymansk/OOF90/bugs.html
http://kitchingroup.cheme.cmu.edu/blog/2014/02/04/Literate-programming-example-with-Fortran-and-org-mode/


* Project proposal
** Summary
 I will implement and optimize an algorithm to compute the Tutte Polynomial for
 arbitrary graphs using both MPI and openMP. I will examine the effect of
 different edge selection heuristics on the performance scaling of the program. 

** Background
 In the search for a proof to the four color theorem, the mathematician George
 David Birkhoff came up with the Chromatic Polynomial which counts the number of
 possible colorings of a graph as a function of $k$ possible colors to color it
 with. If one can show that this polynomial is positive for all planar graphs
 when $k=4$, then one proves the four color theorem. While it didn't prove useful
 in actually proving the four color theorem, W.T. Tutte generalized this
 chromatic polynomial to the Tutte Polynomial which has become an important Graph
 invariant that has been related to results from knot theory, combinatorics, and
 statistical physics. 

 The Tutte Polynomial can be defined recursively using edge contraction ($G/e$)
 and edge deletion ($G - e$) on an undirected graph $G$ as

 \[T_G(x,y) = T_{G-e}(x,y) + T_{G/e}(x,y)\]

 where $e$ must be neither a bridge nor a loop. The base case being when $G$
 contains only $i$ bridges and $j$ loops is $T_G(x,y) = x^iy^j$

 The Tutte polynomial can also be given in closed form for a graph $G = (V, E)$
 by:

 \[T_G(x,y) = \sum_A\subseteq E (x-1)^{k(A) - k(E)}(y-1)^{k(A) + |A| - |V|}\] 

 Where $k(A)$ is the number of connected components of $(V,A)$. 

 From this statement of the Tutte polynomial one can guess that it happens to #P
 hard, as the sum is over the power set of the edges. Evaluating the it a various
 points yields various results of interest, for example $y=0$ gives the chromatic
 polynomial. The Wikipedia page has a good summary of this discussion
 ([[https://en.wikipedia.org/wiki/Tutte_polynomial]]).

** The Challenge

 Between the two definitions given above, the recursive definition is the most
 promising for implementing a fast algorithm. This is because it naturally
 creates a computation tree with potentially redundant computation being
 performed along some branches which we can use Dynamic Programming ideas to
 speedup. The following illustration (taken from the wikipedia article)
 illustrates an example of calculating the Tutte polynomial using the recursive
 deletion-contraction definition.

 #+attr_latex: :width 4in
 [[file:figs/Deletion-contraction.svg]]

 To find branches with redundant computation we need to be able to identify
 isomorphic graphs. One of the most prominent programs for doing so is the nauty
 and Traces program written by Brendan McKay ([[http://pallini.di.uniroma1.it/]]).
 This program can output a canonical labeling for a graph which can then be used
 as a key in a graph cache.

 The interesting issues to consider in paralleling this will be surrounding this
 cache as it is the central part to speeding up this otherwise intractable
 computation. Sharing this cache safely and efficiently across all the processes
 in MPI may be difficult and might benefit from compressing the graphs. Also
 locking the cache data structure within a process in openMP when it needs to be
 updated will be tricky to keep the overhead low.

 Furthermore while each of the two recursive calls can be done in parallel, it is
 not guaranteed that each will have an equal division of work, especially if one
 side ends up having more cache hits than the other, so scheduling will be
 another issue that will likely have to be done dynamically, potentially with
 some type of work queue. Hiding latency can be a potential area for speedups
 since finding the graph isomorphisms is a relatively compute intensive process
 while the cache lookup and additions are a memory intensive process. Finally the
 usual

** Resources
 /Computing Tutte Polynomials/ by Gary Haggard, David J. Pearce, and Gordon Royle
 (2010) will probably be the basis of my implementation as they seem to have the
 ``current'' best implementation. Their code is available on David Pearce’s web-
 site at [[http://homepages.ecs.vuw.ac.nz/~djp/tutte/]] and is implemented in C++.
 This will serve as my reference to check against for both correctness and
 sequential performance, however I will likely rewrite this from scratch in
 fortran (since I'll be spending my summer internship in fortran so I'd like to
 get some practice) to ensure I understand the algorithm completely (it's not a
 very substantial amount of code). Like their implementation, I will use Brendan
 McKay's nauty for finding graph isomorphisms.

 I may also checkout Mathematica's function to compute the Tutte polynomial as
 another benchmark to compare to
 ([[http://reference.wolfram.com/language/ref/TuttePolynomial.html]]).

 Furthermore a more recent paper /A new edge selection heuristic for computing
 the Tutte polynomial of an undirected graph./ by Michael Monagan (2018) proposes
 a different heuristic from those of Haggard, Pearce, Royale that Monagan claims
 offers massive speedups on many types of graphs. His implementation is in Maple
 and is available at [[http://www.cecm.sfu.ca/~mmonagan/tutte/]].

 Finally an older paper /Computing the Tutte Polynomial in Vertex-Exponential
 Time/ by Andreas Björklund, Thore Husfeldt, Petteri Kaski, Mikko Koivisto (2008)
 which is referenced by Haggard, Pearce, Royle has code available here
 [[https://github.com/thorehusfeldt/tutte_bhkk]] along with a nice blog post
 using it here [[http://mhenderson.net/page3/]].

 I think between the latedays cluster and the ghc machines, I will be sufficient
 hardware to perform scaling experiments and performance assessments.

** Goals and Deliverables
*** Plan to achieve
 - A complete working program using MPI capable of correctly computing Tutte
   polynomials at speeds comparable to the reference implementations and with
   good performance scaling .
 - The three edge selection heuristics (2 from Haggard, Pearce, Royle, 1 from
   Monagan) implemented and a comparison of their performance scaling.

*** Hope to achieve
 - Verifying the result listed of Pearce's website of the Tutte polynomial of the
   Truncated Icosahedron.

 Haggard, Pearce, Royle claim it took one week on 150 machines to calculate,
 however Monagan claims it took four minutes on a single cpu using his edge
 selection heuristic. I would like to be able to meet and exceed both of these
 benchmarks for performance however its difficult to specify a speedup since
 Pearce doesn't list the machine specs used, however given this was achieved in
 2009, I expect the specs on the latedays cluster to be much better. As for the
 Monagan result, since he implemented this in Mapel using no isomorphism test, I
 would hope to get at least a 2x speedup (conservative) due to not having the
 overhead of the maple runtime and additional speedup of from additional pruning
 the computation tree due to isomorphisms.

 - An openMP implementation to compare with the MPI implementation.
 - A combined MPI and openMP implementation to compare to the individual
   implementations.

** Platform Choice
 I will use fortran (since I'll be spending my summer internship in fortran so
 I'd like to get some practice) and MPI and openMP and run on the latedays
 cluster and ghc machines. For this type of application MPI is the most important
 to utilize since to have hope to scale to really massive graphs exceeding what
 can be held in memory by a single machine, it is necessary to use a distributed
 model.

** Schedule
 - Week of April 16th
   - Test harness and sequential implementation
   - Understanding of nauty canonical labeling format and code to interface with
     it (necessary for graph cache)
 - Week of April 23rd
   - Parallel MPI implementation with graph cache and Haggard, Pearce, Royle
     heuristics 
 - Week of April 30th
   - Parallel OpenMP implementation with Monagan heuristic.


* Code
** .gitignore
#+begin_src .gitignore :tangle .gitignore
# Ignore everything
*
# except this literate org file
!paralleltuttepoly.org
#+end_src

** Makefile
#+begin_src makefile :tangle Makefile
nauty/nauty.a:
	$(MAKE) -C nauty nauty.a

nauty_test: nauty_test.c nauty/nauty.h nauty/nauty.a 
	gcc nauty_test.c nauty/nauty.a -Inauty -o nauty_test

nauty_thread: nauty_test.c nauty/nauty.h nauty/nauty.a 
	gcc nauty_thread.c nauty/nauty.a -Inauty -o nauty_thread

tutte:	tutte.c
	gcc tutte.c -o tutte

execute: tutte
	./tutte

clean:
	rm -f a.out *.o
#+end_src

** main
#+begin_src c :tangle tutte.c
#include <string.h>
#include <getopt.h>
#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <stdarg.h>
#include <string.h>
#include <ctype.h>
#include <math.h>

void outmsg(char *fmt, ...) {
    va_list ap;
    bool got_newline = fmt[strlen(fmt)-1] == '\n';
    va_start(ap, fmt);
    vfprintf(stderr, fmt, ap);
    va_end(ap);
    if (!got_newline)
	fprintf(stderr, "\n");
}

static void usage(char *name) {
    char *use_string = "-g GFILE -r RFILE [-n STEPS] [-s SEED] [-u (r|b|s)] [-q] [-i INT]";
    outmsg("Usage: %s %s\n", name, use_string);
    outmsg("   -h        Print this message\n");
    outmsg("   -g GFILE  Graph file\n");
    outmsg("   -r RFILE  Initial rat position file\n");
    outmsg("   -n STEPS  Number of simulation steps\n");
    outmsg("   -s SEED   Initial RNG seed\n");
    outmsg("   -u UPDT   Update mode:\n");
    outmsg("             s: Synchronous.  Compute all new states and then update all\n");
    outmsg("             r: Rat order.    Compute update each rat state in sequence\n");
    outmsg("             b: Batched.      Repeatedly compute states for small batches of rats and then update\n");
    outmsg("   -q        Operate in quiet mode.  Do not generate simulation results\n");
    outmsg("   -i INT    Display update interval\n");
}

int main(int argc, char *argv[]) {
  int c;
  char *optstring = "hg:r:R:n:s:u:i:q";
  while ((c = getopt(argc, argv, optstring)) != -1) {
    switch(c) {
    case 'h':
      break;
    case 'q':
      break;
    default:
      usage(argv[0]);
    }
  }
  return 0;
}
#+end_src

#+begin_src c :tangle nauty_test.c
/* This program prints generators for the automorphism group of an
   n-vertex polygon, where n is a number supplied by the user.

   This version uses dynamic allocation.
*/

#include "nauty.h"   
/* MAXN=0 is defined by nauty.h, which implies dynamic allocation */

int
main(int argc, char *argv[])
{
  /* DYNALLSTAT declares a pointer variable (to hold an array when it
     is allocated) and a size variable to remember how big the array is.
     Nothing is allocated yet.  */
 
    DYNALLSTAT(graph,g,g_sz);
    DYNALLSTAT(int,lab,lab_sz);
    DYNALLSTAT(int,ptn,ptn_sz);
    DYNALLSTAT(int,orbits,orbits_sz);
    static DEFAULTOPTIONS_GRAPH(options);
    statsblk stats;

    int n,m,v;
    set *gv;

/* Default options are set by the DEFAULTOPTIONS_GRAPH macro above.
   Here we change those options that we want to be different from the
   defaults.  writeautoms=TRUE causes automorphisms to be written. */

    options.writeautoms = TRUE;

    while (1)
    {
        printf("\nenter n : ");
        if (scanf("%d",&n) == 1 && n > 0)
        {

     /* The nauty parameter m is a value such that an array of
        m setwords is sufficient to hold n bits.  The type setword
        is defined in nauty.h.  The number of bits in a setword is
        WORDSIZE, which is 16, 32 or 64.  Here we calculate
        m = ceiling(n/WORDSIZE). */

            m = SETWORDSNEEDED(n);

         /* The following optional call verifies that we are linking
            to compatible versions of the nauty routines. */

            nauty_check(WORDSIZE,m,n,NAUTYVERSIONID);

         /* Now that we know how big the graph will be, we allocate
          * space for the graph and the other arrays we need. */

            DYNALLOC2(graph,g,g_sz,m,n,"malloc");
            DYNALLOC1(int,lab,lab_sz,n,"malloc");
            DYNALLOC1(int,ptn,ptn_sz,n,"malloc");
            DYNALLOC1(int,orbits,orbits_sz,n,"malloc");

            EMPTYGRAPH(g,m,n);
            for (v = 0; v < n; ++v) ADDONEEDGE(g,v,(v+1)%n,m);

            printf("Generators for Aut(C[%d]):\n",n);
            densenauty(g,lab,ptn,orbits,&options,&stats,m,n,NULL);

            printf("order = ");
            writegroupsize(stdout,stats.grpsize1,stats.grpsize2);
            printf("\n");
        }
        else
            break;
    }

    exit(0);
}
#+end_src

#+begin_src c :tangle nauty_thread.c
/* nauthread1.c

   This program tests dense nauty running in multiple threads.
   It must be linked with nauty as configured with 
   --enable-tls and will only run on systems which support
   thread-local storage.
,*/

#include <pthread.h>
#include "nauty.h"   
/* MAXN=0 is defined by nauty.h, which implies dynamic allocation */

#define THREADS 1000    /* Total number of threads to run */
#define ATONCE 20       /* Number of threads to run at once */
#define GRAPHSIZE 200   /* Least graph size to use */

typedef struct
{
    int n;
    boolean writeautoms;
} params;   /* Used to pass parameters to the thread */

static void*
runit(void * threadarg)          /* Main routine for one thread */
{
    DYNALLSTAT(graph,g,g_sz);
    DYNALLSTAT(int,lab,lab_sz);
    DYNALLSTAT(int,ptn,ptn_sz);
    DYNALLSTAT(int,orbits,orbits_sz);
    DEFAULTOPTIONS_GRAPH(options);
    statsblk stats;
    set *gv;

    int n,m,v;

    n = ((params*)threadarg)->n;

 /* Default options are set by the DEFAULTOPTIONS_GRAPH macro above.
    Here we change those options that we want to be different from the
    defaults.  writeautoms=TRUE causes automorphisms to be written.     */

    options.writeautoms = ((params*)threadarg)->writeautoms;

    m = SETWORDSNEEDED(n);

 /* The following optional call verifies that we are linking
    to compatible versions of the nauty routines.            */

    nauty_check(WORDSIZE,m,n,NAUTYVERSIONID);

    DYNALLOC2(graph,g,g_sz,m,n,"malloc");
    DYNALLOC1(int,lab,lab_sz,n,"malloc");
    DYNALLOC1(int,ptn,ptn_sz,n,"malloc");
    DYNALLOC1(int,orbits,orbits_sz,n,"malloc");

 /* Now we will make a polygon of n vertices */

    EMPTYGRAPH(g,m,n);
    for (v = 0; v < n; ++v) ADDONEEDGE(g,v,(v+1)%n,m);

    if (options.writeautoms)
         printf("Generators for Aut(C[%d]):\n",n);
    densenauty(g,lab,ptn,orbits,&options,&stats,m,n,NULL);

    if (options.writeautoms)
    {
        printf("order = ");
        writegroupsize(stdout,stats.grpsize1,stats.grpsize2);
        printf("\n");
    }
    if (stats.numorbits != 1 || stats.grpsize1 != 2*n)
        fprintf(stderr,">E group error\n");

 /* If we are using multiple threads, we need to free all the dynamic
    memory we have allocated.  We don't have to do this after each 
    call to nauty, just once before the thread finishes. */

    DYNFREE(g,g_sz);
    DYNFREE(lab,lab_sz);
    DYNFREE(ptn,ptn_sz);
    DYNFREE(orbits,orbits_sz);
    nauty_freedyn();
    nautil_freedyn();
    naugraph_freedyn();  /* Use nausparse_freedyn() instead if
                            sparse format is being used. */

    return NULL;
}


int
main(int argc, char *argv[])
{
    int n,ret;
    pthread_t thread[THREADS];
    params par[THREADS];
    int started,finished;

#if !HAVE_TLS
    fprintf(stderr,">E This program needs to be linked with a version\n");
    fprintf(stderr,"  of nauty successfully configured with --enable-tls.\n");
    exit(1);
#endif

    for (started = finished = 0; finished < THREADS; )
    {
	if (started == THREADS || started-finished == ATONCE)
	{
	    if ((ret = pthread_join(thread[finished],NULL)) != 0)
	    {
                fprintf(stderr,">E Thread joining failed, code=%d\n",ret);    
                exit(1);
            }
	    ++finished;
	}
	else
	{
		/* We vary the graph size a bit as it tests the
                   thread independence better. */
	    par[started].n = GRAPHSIZE + (started % 17);
	    par[started].writeautoms = FALSE;

            if ((ret = pthread_create(&thread[started],NULL,
					runit,&par[started])) != 0)
            {
                fprintf(stderr,">E Thread creation failed, code=%d\n",ret);       
                exit(1);
            }
	    ++started;
	}
    }

    fprintf(stderr,"sucess?");
    exit(0);
}
#+end_src
